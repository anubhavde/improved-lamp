{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Playing TicTacToe with Reinforcement Learning & OpenAI Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-09 09:09:12--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/gym-tictactoe.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10119 (9.9K) [application/zip]\n",
      "Saving to: ‘gym-tictactoe.zip’\n",
      "\n",
      "gym-tictactoe.zip   100%[===================>]   9.88K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-07-09 09:09:14 (91.2 MB/s) - ‘gym-tictactoe.zip’ saved [10119/10119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/gym-tictactoe.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  gym-tictactoe.zip\n",
      "   creating: gym-tictactoe/\n",
      "   creating: gym-tictactoe/gym_tictactoe.egg-info/\n",
      "   creating: gym-tictactoe/gym_tictactoe/\n",
      "  inflating: gym-tictactoe/setup.py  \n",
      "   creating: gym-tictactoe/.ipynb_checkpoints/\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/PKG-INFO  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/SOURCES.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/requires.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/top_level.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/dependency_links.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/__init__.py  \n",
      "   creating: gym-tictactoe/gym_tictactoe/__pycache__/\n",
      "   creating: gym-tictactoe/gym_tictactoe/.ipynb_checkpoints/\n",
      "   creating: gym-tictactoe/gym_tictactoe/envs/\n",
      "  inflating: gym-tictactoe/.ipynb_checkpoints/setup-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/__pycache__/__init__.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/.ipynb_checkpoints/__init__-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__init__.py  \n",
      "   creating: gym-tictactoe/gym_tictactoe/envs/__pycache__/\n",
      "   creating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/tictactoe_env.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/__init__.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/tictactoe_env.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/__init__-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/tictactoe_env-checkpoint.py  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o gym-tictactoe.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import gym_tictactoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TicTacToe-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(0, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X--------'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['X', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XO-------', (0, 0)),\n",
       " ('X-O------', (0, 0)),\n",
       " ('X--O-----', (0, 0)),\n",
       " ('X---O----', (0, 0)),\n",
       " ('X----O---', (0, 0)),\n",
       " ('X-----O--', (0, 0)),\n",
       " ('X------O-', (0, 0)),\n",
       " ('X-------O', (0, 0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_states(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, (0, 0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.check_done(env.hash())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', 'X', 'O']\n",
      "['X', '-', 'X']\n",
      "(10, -10)\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    env.render()\n",
    "\n",
    "    if not done:\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "        env.render()\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---X-----\n",
      "---X--O--\n",
      "X--X--O--\n",
      "X-OX--O--\n",
      "X-OX--O-X\n",
      "X-OX--OOX\n",
      "XXOX--OOX\n",
      "XXOXO-OOX\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    print(env.hash())\n",
    "\n",
    "    if not done:\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "        print(env.hash())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, player=\"X\", alpha=0.4, gamma=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.player = player\n",
    "        self.player_number = 0 if player == \"X\" else 1\n",
    "        self.V = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def select_action(self, epsilon=0.1):\n",
    "        if (random.random() < epsilon):\n",
    "            action = random.choice(self.env.available_actions())\n",
    "        else:\n",
    "            q_values = []\n",
    "            for state in self.env.available_states(self.player):\n",
    "                q_values.append(self.gamma*self.V[state[0]] + state[1][self.player_number])\n",
    "            \n",
    "            max_value = max(q_values)\n",
    "            max_indexs = [i for i, j in enumerate(q_values) if j == max_value]\n",
    "            action = self.env.available_actions()[random.choice(max_indexs)]\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def add_states(self):\n",
    "        if (self.env.hash() not in self.V):\n",
    "            self.V[self.env.hash()] = 0\n",
    "        for state in self.env.available_states(\"X\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0\n",
    "\n",
    "        for state in self.env.available_states(\"O\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def update_state_values(self, new_state, old_state, reward):\n",
    "        self.V[old_state] = self.V[old_state] + self.alpha*(reward + self.gamma*self.V[new_state] - self.V[old_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "    agent_x = Agent(env, \"X\")\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()\n",
    "            action = agent_x.select_action()\n",
    "            new_state, reward, done, _ = env.step(action, agent_x.player)\n",
    "            agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "            if not done:\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()\n",
    "                action = agent_o.select_action()\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)\n",
    "                agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_x, agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 58.1 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "agent_x, agent_o = train(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, agent_x.player)\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                o_action = random.choice(env.available_actions())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "        \n",
    "        if (reward == (10, -10)):\n",
    "            win += 1\n",
    "        elif (reward == (-10, 10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9902 Tie: 98 Loss: 0\n",
      "Win Rate: 99.02 Tie Rate: 0.98 Loss Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_x(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_o(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            x_action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, agent_o.player)\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 8520 Tie: 1473 Loss: 7\n",
      "Win Rate: 85.2 Tie Rate: 14.729999999999999 Loss Rate: 0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episodes):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return x_win, o_win, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win: 0 Tie: 10000 O Win: 0\n",
      "X Win Rate: 0.0 Tie Rate: 100.0 O Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "x_win, o_win, tie = test(episodes)\n",
    "\n",
    "print(\"X Win:\", x_win, \"Tie:\", tie, \"O Win:\", o_win)\n",
    "print(\"X Win Rate:\", x_win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"O Win Rate:\", o_win/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_x(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(env.available_actions())\n",
    "            agent_o.add_states()\n",
    "            x_action = int(input())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Board\n",
      "['-', 'X', '-']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[0, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'O', '-']\n",
      "['-', '-', 'O']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_o(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                env.render()\n",
    "                print(env.available_actions())\n",
    "                agent_x.add_states()\n",
    "                o_action = int(input())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_o_against_random(episodes):\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()\n",
    "            action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(action, \"X\")\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "            if not done:\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()\n",
    "                action = agent_o.select_action()\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_o = train_o_against_random(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9094 Tie: 731 Loss: 175\n",
      "Win Rate: 90.94 Tie Rate: 7.31 Loss Rate: 1.7500000000000002\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rapids-22.02')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86468c90cac0f8646058a0a909ee741a583b9966e79e5a9e5d3dcdec03b52931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
