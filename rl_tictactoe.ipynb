{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Playing TicTacToe with Reinforcement Learning & OpenAI Gym**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the necessary modules required i.e gym(to initialize & work with the TicTacToe environment), random(to make random choices when interacting with the environment) & gym_tictactoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import gym_tictactoe # this is our tictactoe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the installed environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TicTacToe-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(0, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X--------'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will render the current state of the game in the form of a TicTacToe board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['X', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of positions available based on the current state of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of tuples representing the states and rewards a possible player can get based on the current state of the game. This function will be used to see what is possible to get to and what reward they give which will be used in decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XO-------', (0, 0)),\n",
       " ('X-O------', (0, 0)),\n",
       " ('X--O-----', (0, 0)),\n",
       " ('X---O----', (0, 0)),\n",
       " ('X----O---', (0, 0)),\n",
       " ('X-----O--', (0, 0)),\n",
       " ('X------O-', (0, 0)),\n",
       " ('X-------O', (0, 0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_states(\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check whether or not the game is done in form of boolean and a return a reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, (0, 0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.check_done(env.hash())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the board so a new game can be played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', 'X', 'O']\n",
      "['X', '-', 'X']\n",
      "(10, -10)\n"
     ]
    }
   ],
   "source": [
    "done = False        # variable to keep track of whether game is over or not\n",
    "env.reset()     # reset environment to clear any old game\n",
    "env.render()        # print the initial board\n",
    "while not done:     # play game until it is over\n",
    "    # make a random action from list of available_actions for X\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    env.render()        # print board after X's action\n",
    "\n",
    "    if not done:        # if game is done on X action we don't want O to make an action\n",
    "        # make a random action from list of available_actions for O\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "            # print board after O action\n",
    "        env.render()\n",
    "\n",
    "print(reward)       # print reward after game is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario that the agent can get into with respect to the environment, below are all examples of states in TicTacToe remember we are using the hash of the board as the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---X-----\n",
      "---X--O--\n",
      "X--X--O--\n",
      "X-OX--O--\n",
      "X-OX--O-X\n",
      "X-OX--OOX\n",
      "XXOX--OOX\n",
      "XXOXO-OOX\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    print(env.hash())\n",
    "\n",
    "    if not done:\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "        print(env.hash())       # print state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Temporal Difference Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, player=\"X\", alpha=0.4, gamma=0.9):      # default alpha & gamma values determined from testing\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env      # reference to environment agent is in\n",
    "        self.player = player\n",
    "        self.player_number = 0 if player == \"X\" else 1        # index of the reward tuple agent uses\n",
    "        self.V = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to select an action to take using Epsilon Greedy Policy where epsilon defines how often the policy will make a random action   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def select_action(self, epsilon=0.1):       # epsilon default value determined from testing \n",
    "        if (random.random() < epsilon):     # greedy action taken if random number between 0 and 1 below epsilon value\n",
    "            action = random.choice(self.env.available_actions())        # random action from list of available_actions\n",
    "        else:\n",
    "            q_values = []       # list to store action calculations \n",
    "            for state in self.env.available_states(self.player):\n",
    "                q_values.append(self.gamma*self.V[state[0]] + state[1][self.player_number])\n",
    "            \n",
    "            max_value = max(q_values)       # max value of action calculations\n",
    "            max_indexs = [i for i, j in enumerate(q_values) if j == max_value]      # index of max_value in q_values\n",
    "            action = self.env.available_actions()[random.choice(max_indexs)]        # random action from actions that all have the max_value\n",
    "        \n",
    "        return action       # integer representing which action to take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to add states that are not in the state value dictionary and initialize them with the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def add_states(self):\n",
    "        # adds current state to state value function\n",
    "        if (self.env.hash() not in self.V):\n",
    "            self.V[self.env.hash()] = 0\n",
    "        \n",
    "        # adds all states X can get to\n",
    "        for state in self.env.available_states(\"X\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0\n",
    "\n",
    "        # adds all states O can get to\n",
    "        for state in self.env.available_states(\"O\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to update state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def update_state_values(self, new_state, old_state, reward):\n",
    "        self.V[old_state] = self.V[old_state] + self.alpha*(reward + self.gamma*self.V[new_state] - self.V[old_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train our agent in the environment. In each turn; states for both agents will be added, an action will be selected & the step will be performed with a specific agent. Finally we will update values for both agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes):        # episodes stands for the number of games played\n",
    "    # creating our agents\n",
    "    agent_x = Agent(env, \"X\")\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False        # stop when game is done\n",
    "        env.reset()     # reset environment\n",
    "        while not done:     # loop for a single game\n",
    "            # X agents' turn\n",
    "            # add states for both agents    \n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()      # record state\n",
    "            action = agent_x.select_action()        # get action using policy\n",
    "            new_state, reward, done, _ = env.step(action, agent_x.player)       # perform action\n",
    "            # update state values for both agents\n",
    "            agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "            if not done:        # don't make O move if game ends on X move\n",
    "                # O agents turn\n",
    "                # add states for both agents\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()      # record state\n",
    "                action = agent_o.select_action()        # get action using policy\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)       # perform action\n",
    "                # update state values for both agents\n",
    "                agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_x, agent_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our agents on 110,000 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 58.1 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "agent_x, agent_o = train(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, agent_x.player)\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                o_action = random.choice(env.available_actions())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "        \n",
    "        if (reward == (10, -10)):\n",
    "            win += 1\n",
    "        elif (reward == (-10, 10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9902 Tie: 98 Loss: 0\n",
      "Win Rate: 99.02 Tie Rate: 0.98 Loss Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_x(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_o(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            x_action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, agent_o.player)\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 8520 Tie: 1473 Loss: 7\n",
      "Win Rate: 85.2 Tie Rate: 14.729999999999999 Loss Rate: 0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episodes):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return x_win, o_win, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win: 0 Tie: 10000 O Win: 0\n",
      "X Win Rate: 0.0 Tie Rate: 100.0 O Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "x_win, o_win, tie = test(episodes)\n",
    "\n",
    "print(\"X Win:\", x_win, \"Tie:\", tie, \"O Win:\", o_win)\n",
    "print(\"X Win Rate:\", x_win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"O Win Rate:\", o_win/episodes*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing against our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_x(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(env.available_actions())\n",
    "            agent_o.add_states()\n",
    "            x_action = int(input())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Board\n",
      "['-', 'X', '-']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[0, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'O', '-']\n",
      "['-', '-', 'O']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_o(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                env.render()\n",
    "                print(env.available_actions())\n",
    "                agent_x.add_states()\n",
    "                o_action = int(input())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_o_against_random(episodes):\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()\n",
    "            action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(action, \"X\")\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "            if not done:\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()\n",
    "                action = agent_o.select_action()\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_o = train_o_against_random(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9094 Tie: 731 Loss: 175\n",
      "Win Rate: 90.94 Tie Rate: 7.31 Loss Rate: 1.7500000000000002\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rapids-22.02')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86468c90cac0f8646058a0a909ee741a583b9966e79e5a9e5d3dcdec03b52931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
