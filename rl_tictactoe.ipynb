{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Playing TicTacToe with Reinforcement Learning & OpenAI Gym**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the necessary modules required i.e gym(to initialize & work with the TicTacToe environment), random(to make random choices when interacting with the environment) & gym_tictactoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import gym_tictactoe # this is our tictactoe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the installed environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TicTacToe-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(0, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X--------'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will render the current state of the game in the form of a TicTacToe board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['X', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of positions available based on the current state of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of tuples representing the states and rewards a possible player can get based on the current state of the game. This function will be used to see what is possible to get to and what reward they give which will be used in decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XO-------', (0, 0)),\n",
       " ('X-O------', (0, 0)),\n",
       " ('X--O-----', (0, 0)),\n",
       " ('X---O----', (0, 0)),\n",
       " ('X----O---', (0, 0)),\n",
       " ('X-----O--', (0, 0)),\n",
       " ('X------O-', (0, 0)),\n",
       " ('X-------O', (0, 0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_states(\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check whether or not the game is done in form of boolean and a return a reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, (0, 0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.check_done(env.hash())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the board so a new game can be played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', '-', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', '-', 'O']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', 'O', '-']\n",
      "['O', 'X', 'O']\n",
      "['X', '-', 'X']\n",
      "(10, -10)\n"
     ]
    }
   ],
   "source": [
    "done = False        # variable to keep track of whether game is over or not\n",
    "env.reset()     # reset environment to clear any old game\n",
    "env.render()        # print the initial board\n",
    "while not done:     # play game until it is over\n",
    "    # make a random action from list of available_actions for X\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    env.render()        # print board after X's action\n",
    "\n",
    "    if not done:        # if game is done on X action we don't want O to make an action\n",
    "        # make a random action from list of available_actions for O\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "            # print board after O action\n",
    "        env.render()\n",
    "\n",
    "print(reward)       # print reward after game is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario that the agent can get into with respect to the environment, below are all examples of states in TicTacToe remember we are using the hash of the board as the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---X-----\n",
      "---X--O--\n",
      "X--X--O--\n",
      "X-OX--O--\n",
      "X-OX--O-X\n",
      "X-OX--OOX\n",
      "XXOX--OOX\n",
      "XXOXO-OOX\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    new_state, reward, done, info = env.step(\n",
    "        random.choice(env.available_actions()), \"X\")\n",
    "    print(env.hash())\n",
    "\n",
    "    if not done:\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random.choice(env.available_actions()), \"O\")\n",
    "        print(env.hash())       # print state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Temporal Difference Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, player=\"X\", alpha=0.4, gamma=0.9):      # default alpha & gamma values determined from testing\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env      # reference to environment agent is in\n",
    "        self.player = player\n",
    "        self.player_number = 0 if player == \"X\" else 1        # index of the reward tuple agent uses\n",
    "        self.V = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def select_action(self, epsilon=0.1):\n",
    "        if (random.random() < epsilon):\n",
    "            action = random.choice(self.env.available_actions())\n",
    "        else:\n",
    "            q_values = []\n",
    "            for state in self.env.available_states(self.player):\n",
    "                q_values.append(self.gamma*self.V[state[0]] + state[1][self.player_number])\n",
    "            \n",
    "            max_value = max(q_values)\n",
    "            max_indexs = [i for i, j in enumerate(q_values) if j == max_value]\n",
    "            action = self.env.available_actions()[random.choice(max_indexs)]\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def add_states(self):\n",
    "        if (self.env.hash() not in self.V):\n",
    "            self.V[self.env.hash()] = 0\n",
    "        for state in self.env.available_states(\"X\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0\n",
    "\n",
    "        for state in self.env.available_states(\"O\"):\n",
    "            if (state[0] not in self.V):\n",
    "                self.V[state[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def update_state_values(self, new_state, old_state, reward):\n",
    "        self.V[old_state] = self.V[old_state] + self.alpha*(reward + self.gamma*self.V[new_state] - self.V[old_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "    agent_x = Agent(env, \"X\")\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()\n",
    "            action = agent_x.select_action()\n",
    "            new_state, reward, done, _ = env.step(action, agent_x.player)\n",
    "            agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "            if not done:\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()\n",
    "                action = agent_o.select_action()\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)\n",
    "                agent_x.update_state_values(new_state, old_state, reward[agent_x.player_number])\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_x, agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 58.1 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "agent_x, agent_o = train(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, agent_x.player)\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                o_action = random.choice(env.available_actions())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "        \n",
    "        if (reward == (10, -10)):\n",
    "            win += 1\n",
    "        elif (reward == (-10, 10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9902 Tie: 98 Loss: 0\n",
      "Win Rate: 99.02 Tie Rate: 0.98 Loss Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_x(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_o(episodes):\n",
    "    win = 0\n",
    "    tie = 0\n",
    "    loss = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            x_action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, agent_o.player)\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            loss += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return win, loss, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 8520 Tie: 1473 Loss: 7\n",
      "Win Rate: 85.2 Tie Rate: 14.729999999999999 Loss Rate: 0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episodes):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            agent_o.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_x.add_states()\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return x_win, o_win, tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win: 0 Tie: 10000 O Win: 0\n",
      "X Win Rate: 0.0 Tie Rate: 100.0 O Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "x_win, o_win, tie = test(episodes)\n",
    "\n",
    "print(\"X Win:\", x_win, \"Tie:\", tie, \"O Win:\", o_win)\n",
    "print(\"X Win Rate:\", x_win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"O Win Rate:\", o_win/episodes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_x(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            print(env.available_actions())\n",
    "            agent_o.add_states()\n",
    "            x_action = int(input())\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                agent_o.add_states()\n",
    "                o_action = agent_o.select_action(epsilon=0)\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Board\n",
      "['-', 'X', '-']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[0, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['-', 'O', '-']\n",
      "['-', '-', '-']\n",
      "[3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'O', '-']\n",
      "['-', '-', 'O']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_as_o(episodes=1):\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_x.add_states()\n",
    "            x_action = agent_x.select_action(epsilon=0)\n",
    "            new_state, reward, done, _ = env.step(x_action, \"X\")\n",
    "            if (not done):\n",
    "                env.render()\n",
    "                print(env.available_actions())\n",
    "                agent_x.add_states()\n",
    "                o_action = int(input())\n",
    "                new_state, reward, done, _ = env.step(o_action, \"O\")\n",
    "\n",
    "        env.render()\n",
    "        if (reward == (-10, 10)):\n",
    "            print(\"You Win\")\n",
    "        elif (reward == (10, -10)):\n",
    "            print(\"You Lose\")\n",
    "        elif (reward == (0, 0)):\n",
    "            print(\"Tie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[0, 1, 2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "[2, 3, 5, 6, 7, 8]\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['O', 'X', '-']\n",
      "['-', 'X', '-']\n",
      "You Lose\n"
     ]
    }
   ],
   "source": [
    "play_as_o()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_o_against_random(episodes):\n",
    "    agent_o = Agent(env, \"O\")\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            agent_o.add_states()\n",
    "            old_state = env.hash()\n",
    "            action = random.choice(env.available_actions())\n",
    "            new_state, reward, done, _ = env.step(action, \"X\")\n",
    "            agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "            if not done:\n",
    "                agent_o.add_states()\n",
    "                old_state = env.hash()\n",
    "                action = agent_o.select_action()\n",
    "                new_state, reward, done, _ = env.step(action, agent_o.player)\n",
    "                agent_o.update_state_values(new_state, old_state, reward[agent_o.player_number])\n",
    "\n",
    "    return agent_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_o = train_o_against_random(110000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win: 9094 Tie: 731 Loss: 175\n",
      "Win Rate: 90.94 Tie Rate: 7.31 Loss Rate: 1.7500000000000002\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "win, loss, tie = test_o(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie / episodes*100, \"Loss Rate:\", loss/episodes*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rapids-22.02')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86468c90cac0f8646058a0a909ee741a583b9966e79e5a9e5d3dcdec03b52931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
